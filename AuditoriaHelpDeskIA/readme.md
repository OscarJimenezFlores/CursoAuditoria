# Corporate EPIS Pilot

Asistente de IA conversacional para entornos empresariales, creado para responder dudas de los usuarios con base en una fuente de conocimiento interna y guiar al usuario a trav茅s de un flujo de soluci贸n de problemas antes de crear tickets de soporte t茅cnico.

---

## Caracter铆sticas Principales

- **Arquitectura RAG (Retrieval-Augmented Generation):** El bot basa su conocimiento en un conjunto de documentos privados (PDFs, TXTs) para garantizar respuestas precisas y reducir alucinaciones.
- **Router de Intenciones:** Un LLM clasifica la intenci贸n del usuario (`pregunta general`, `reporte de problema`, `despedida`) para dirigir la conversaci贸n de forma inteligente.
- **Flujo de Conversaci贸n Guiado:** En lugar de crear un ticket directamente, el bot primero ofrece una soluci贸n de la base de conocimiento. Luego, pregunta expl铆citamente al usuario si el problema se ha solucionado, implementando un **bucle de feedback** efectivo.
- **Creaci贸n de Tickets por Acci贸n:** Si la soluci贸n no es suficiente, el frontend ofrece al usuario la opci贸n de crear un ticket. Esta decisi贸n se comunica al backend mediante un mensaje de acci贸n especial (`ACTION_CREATE_TICKET`), demostrando un patr贸n de dise帽o robusto para agentes de IA.
- **Pila Tecnol贸gica Local y Open-Source:** El sistema funciona 100% localmente usando Ollama con Llama 3.1 y modelos de embeddings de alto rendimiento, sin depender de APIs de pago.
- **Listo para Despliegue (Docker y Kubernetes):** El proyecto est谩 completamente "dockerizado" y cuenta con manifiestos de Kubernetes para su orquestaci贸n, demostrando un flujo de trabajo listo para producci贸n.
- **CI/CD Automatizado:** Un workflow de GitHub Actions se encarga de construir y publicar autom谩ticamente las im谩genes de Docker en Docker Hub cada vez que se actualiza el c贸digo.

---

## Arquitectura del Sistema

La arquitectura consiste en un frontend que gestiona el estado de la conversaci贸n y un backend stateless que responde a cada pregunta a trav茅s de un 煤nico endpoint. La l贸gica de decisi贸n reside en el router de LangChain.

```mermaid
graph TD
    A[Usuario] --> B{Frontend React MUI};
    B --> C{Ingress Kubernetes};
    C -- ruta / --> B;
    C -- ruta /api/ask --> D{Backend FastAPI};
    D --> E[Router LangChain];
    E -- Intencion: pregunta_general --> F[RAG Chain];
    E -- Intencion: reporte_de_problema --> F;
    E -- Intencion: despedida --> G[Respuesta Directa];
    F --> H[(Vector Store - ChromaDB)];
    D -- ACTION_CREATE_TICKET --> I[Funcion de Tickets];
    I --> J[(DB - SQLite)];
    H --> K[Documentos PDF TXT];
``` 

## Stack Tecnol贸gico

| rea | Tecnolog铆as |
|------|--------------|
| **Backend** | Python, FastAPI, LangChain, Ollama (Llama 3.1), Uvicorn |
| **IA & NLP** | RAG, Hugging Face Embeddings (`multilingual-e5-large`), ChromaDB |
| **Frontend** | React, TypeScript, Material-UI (MUI), Vite |
| **Base de Datos** | SQLite |
| **DevOps** | Docker, Docker Compose, Kubernetes, NGINX Ingress, GitHub Actions |

## C贸mo Empezar

### Prerrequisitos

- **Docker Desktop** instalado y funcionando (con Kubernetes activado).  
- **Ollama** instalado y ejecut谩ndose, con el modelo `llama3.1:8b` descargado:

```bash
ollama pull llama3.1:8b
```
### Docker

#### Clona el repositorio

```bash
git clone https://github.com/tu-usuario/tu-repositorio.git
cd tu-repositorio
```
#### Clona el repositorio

```bash
docker-compose up --build
```
Accede a la interfaz  http://localhost:5173
